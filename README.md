# Awesome-Visual-Question-Answering-VQA

<img src="VQA.jpg"></img>

A reading list (and code) of resources dedicated to visual(image/video) question answering.

<p><strong><span style="color: red;">Papers and Codes</span></strong></p>
<table style="height: 225px; width: 439px;">
<tbody>
<tr>
<td style="width: 28px;">&nbsp;</td>
<td style="width: 172px;"><strong>Title</strong></td>
<td style="width: 80px;"><strong>Document</strong></td>
<td style="width: 70px; text-align: center;"><strong>Code</strong></td>
<td style="width: 55px;"><strong>Year</strong></td>
</tr>
<tr>
<td style="width: 28px;">&nbsp;1</td>
<td style="width: 172px;">Yan Zhang, Jonathon Hare, Adam Pr&uuml;gel-Bennett:&nbsp;<strong>Learning to Count Objects in Natural Images for Visual Question Answering</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1802.05766" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/Cyanogenoid/vqa-counting">[Code]</a></td>
<td style="width: 55px;">2018</td>
</tr>
<tr>
<td style="width: 28px;">2&nbsp;</td>
<td style="width: 172px;">Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh,&nbsp;<strong>VQA: Visual Question Answering</strong>, ICCV, 2015.</td>
<td style="width: 80px;"><a href="http://arxiv.org/pdf/1505.00468" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/JamesChuanggg/VQA-tensorflow">[Code]</a></td>
<td style="width: 55px;">2015</td>
</tr>
<tr>
<td style="width: 28px;">3&nbsp;</td>
<td style="width: 172px;">Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola,&nbsp;<strong>Stacked Attention Networks for Image Question Answering</strong>, CVPR 2016.&nbsp;</td>
<td style="width: 80px;"><a href="http://arxiv.org/abs/1511.02274" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/JamesChuanggg/san-torch">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">4&nbsp;</td>
<td style="width: 172px;">
<p>Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh,&nbsp;<strong>Hierarchical Question-Image Co-Attention for Visual Question Answering</strong>, arXiv:1606.00061, 2016.&nbsp;&nbsp;</p>
</td>
<td style="width: 80px;"><a href="https://arxiv.org/pdf/1606.00061v2.pdf" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/jiasenlu/HieCoAttenVQA">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">5&nbsp;</td>
<td style="width: 172px;">
<p>Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach,&nbsp;<strong>Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</strong>, arXiv:1606.01847, 2016.&nbsp;</p>
</td>
<td style="width: 80px;"><a href="https://arxiv.org/abs/1606.01847" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/akirafukui/vqa-mcb">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">6&nbsp;</td>
<td style="width: 172px;">
<p>Vahid Kazemi, Ali Elqursh,&nbsp;<strong>Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering</strong>, arXiv:1704.03162, 2016.&nbsp;</p>
</td>
<td style="width: 80px;"><a href="https://arxiv.org/abs/1704.03162" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/Cyanogenoid/pytorch-vqa">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">7&nbsp;</td>
<td style="width: 172px;">
<p>Hedi Ben-younes, Remi Cadene, Matthieu Cord, Nicolas Thome:&nbsp;<strong>MUTAN: Multimodal Tucker Fusion for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/pdf/1705.06676.pdf" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/Cadene/vqa.pytorch">[Code]</a></td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">8&nbsp;</td>
<td style="width: 172px;">
<p><strong>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/pdf/1502.05698v1.pdf" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/facebook/bAbI-tasks">[Code]</a></td>
<td style="width: 55px;">2015</td>
</tr>
<tr>
<td style="width: 28px;">9&nbsp;</td>
<td style="width: 172px;"><strong>Neural Module Networks</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/pdf/1502.05698v1.pdf" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://arxiv.org/pdf/1502.05698v1.pdf" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">10&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/pdf/1511.05756" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/HyeonwooNoh/DPPnet" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2015</td>
</tr>
<tr>
<td style="width: 28px;">11&nbsp;</td>
<td style="width: 172px;">
<p><strong>Stacked Attention Networks for Image Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="http://arxiv.org/abs/1511.02274" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href=" https://github.com/abhshkdz/neural-vqa-attention" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">12&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Simple Baseline for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="http://arxiv.org/abs/1512.02167" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/metalbubble/VQAbaseline" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2015</td>
</tr>
<tr>
<td style="width: 28px;">13&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Question Answering via Integer Programming over Semi-Structured Knowledge</strong></p>
</td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="http://arxiv.org/abs/1604.06076" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/allenai/tableilp" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">14&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Hierarchical Question-Image Co-Attention for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="http://arxiv.org/abs/1606.00061" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/jiasenlu/HieCoAttenVQA" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">15&nbsp;</td>
<td style="width: 172px;">
<p><strong>Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</strong></p>
</td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="https://arxiv.org/abs/1606.01847" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/akirafukui/vqa-mcb" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">16&nbsp;</td>
<td style="width: 172px;"><strong>Hadamard Product for Low-rank Bilinear Pooling</strong></td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="https://arxiv.org/abs/1610.04325" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/jnhwkim/MulLowBiVQA" rel="nofollow">[Code]</a>&nbsp;</td>
<td style="width: 55px;">
<p>2017</p>
</td>
</tr>
<tr>
<td style="width: 28px;">17&nbsp;</td>
<td style="width: 172px;"><strong>TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering</strong></td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="https://arxiv.org/abs/1704.04497" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/YunseokJANG/tgif-qa" rel="nofollow">[Code]</a>&nbsp;</td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">18&nbsp;</td>
<td style="width: 172px;"><strong>Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks</strong></td>
<td style="width: 80px;">&nbsp;&nbsp;<a href=" https://arxiv.org/abs/1704.08384" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/rajarshd/TextKBQA" rel="nofollow">[Code]</a>&nbsp;</td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">19&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Learning Convolutional Text Representations for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="https://arxiv.org/abs/1705.06824" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/divelab/vqa-text" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2018</td>
</tr>
<tr>
<td style="width: 28px;">20&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href=" https://arxiv.org/abs/1707.07998" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;<a href=" https://github.com//peteanderson80/bottom-up-attention" rel="nofollow">[Code]</a>&nbsp;</td>
<td style="width: 55px;">2018</td>
</tr>
<tr>
<td style="width: 28px;">21&nbsp;</td>
<td style="width: 172px;">
<p><strong>Structured Attentions for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1708.02071" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href=" https://github.com/zhuchen03/vqa-sva" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">&nbsp;2017</td>
</tr>
<tr>
<td style="width: 28px;">22&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Question Dependent Recurrent Entity Network for Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1707.07922" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/andreamad8/QDREN" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">23&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>DCN+: Mixed Objective and Deep Residual Coattention for Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1707.07922" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://arxiv.org/abs/1707.07922" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">24&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Embodied Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="https://arxiv.org/abs/1711.11543" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/facebookresearch/EmbodiedQA" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">25&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Structured Triplet Learning with POS-tag Guided Attention for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1801.07853" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/wangzheallen/STL-VQA" rel="nofollow">[Paper]</a></td>
<td style="width: 55px;">2018</td>
</tr>
<tr>
<td style="width: 28px;">26&nbsp;</td>
<td style="width: 172px;"><strong>Bilinear Attention Networks</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1805.07932" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/jnhwkim/ban-vqa" rel="nofollow">[Code]</a>&nbsp;</td>
<td style="width: 55px;">2018</td>
</tr>
<tr>
<td style="width: 28px;">27&nbsp;</td>
<td style="width: 172px;">&nbsp;</td>
<td style="width: 80px;">&nbsp;</td>
<td style="width: 70px;">&nbsp;</td>
<td style="width: 55px;">&nbsp;</td>
</tr>
<tr>
<td style="width: 28px;">&nbsp;</td>
<td style="width: 172px;">&nbsp;</td>
<td style="width: 80px;">&nbsp;</td>
<td style="width: 70px;">&nbsp;</td>
<td style="width: 55px;">&nbsp;</td>
</tr>
<tr>
<td style="width: 28px;">&nbsp;</td>
<td style="width: 172px;">&nbsp;</td>
<td style="width: 80px;">&nbsp;</td>
<td style="width: 70px;">&nbsp;</td>
<td style="width: 55px;">&nbsp;</td>
</tr>
</tbody>
</table>
<p><strong><span style="color: red;">Projects</span></strong></p>
<table style="height: 147px; width: 440px;">
<tbody>
<tr>
<td style="width: 29px;">&nbsp;</td>
<td style="width: 258px; text-align: center;"><strong>&nbsp;Title</strong></td>
<td style="width: 141px; text-align: center;"><strong>Code&nbsp;</strong></td>
</tr>
<tr>
<td style="width: 29px;">&nbsp;1</td>
<td style="width: 258px;"><strong>VQA Demo: Visual Question Answering Demo on pretrained model</strong></td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/iamaaditya/VQA_Demo" rel="nofollow">[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">2&nbsp;</td>
<td style="width: 258px;"><strong>Deep-QA: Implementation of the Convolution Neural Network for factoid QA on the answer sentence selection task</strong></td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/aseveryn/deep-qa" rel="nofollow">[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">3&nbsp;</td>
<td style="width: 258px;"><strong>InsuranceQA-CNN-LSTM: Tensorflow and Theano CNN code for insurance QA(question Answer matching)</strong></td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/white127/insuranceQA-cnn-lstm" rel="nofollow">[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">4&nbsp;</td>
<td style="width: 258px;">&nbsp;
<p><strong>Tensorflow Implementation of Deeper LSTM+ normalized CNN for Visual Question Answering</strong></p>
</td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/JamesChuanggg/VQA-tensorflow" rel="nofollow">[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">5&nbsp;</td>
<td style="width: 258px;">&nbsp;
<p><strong>Visual Question Answering with Keras</strong></p>
</td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering" rel="nofollow">[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">6&nbsp;</td>
<td style="width: 258px;">
<p><strong>Visual Question Answering in Pytorch</strong></p>
</td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/Cadene/vqa.pytorch" rel="nofollow">[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">7&nbsp;</td>
<td style="width: 258px;">
<p><strong>Deep QA: Using deep learning to answer Aristo&rsquo;s science questions</strong></p>
</td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/allenai/deep_qa" rel="nofollow">[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">&nbsp;</td>
<td style="width: 258px;">&nbsp;</td>
<td style="width: 141px;">&nbsp;</td>
</tr>
</tbody>
</table>
