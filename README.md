# Awesome-Visual-Question-Answering-VQA

<img src="VQA.jpg"></img>

A reading list (and code) of resources dedicated to visual question answering.

<strong>Contributing</strong>

Please feel free to send me email (pekermusa@gmail.com) to add links.

<p><strong><span style="color: red;">Papers and Codes</span></strong></p>
<table style="height: 225px; width: 439px;">
<tbody>
  
<tr>
<td style="width: 28px;">&nbsp;</td>
<td style="width: 172px;"><strong>Title</strong></td>
<td style="width: 80px;"><strong>Document</strong></td>
<td style="width: 70px; text-align: center;"><strong>Code</strong></td>
<td style="width: 55px;"><strong>Year</strong></td>
</tr>
  
  
<tr>
<td style="width: 28px;">&nbsp;1</td>
<td style="width: 172px;"><strong>Deep Modular Co-Attention Networks for Visual Question Answering (CVPR 2019)</strong></td>
<td style="width: 80px;">&nbsp;<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.pdf">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/MILVLG/mcan-vqa">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>


<tr>
<td style="width: 28px;">&nbsp;2</td>
<td style="width: 172px;">Yan Zhang, Jonathon Hare, Adam Pr&uuml;gel-Bennett:&nbsp;<strong>Learning to Count Objects in Natural Images for Visual Question Answering</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1802.05766">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/Cyanogenoid/vqa-counting">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>


<tr>
<td style="width: 28px;">&nbsp;3</td>
<td style="width: 172px;"><strong>Video Relationship Reasoning using Gated Spatio-Temporal Energy Graph (CVPR 2019)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1903.10547">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/yaohungt/Gated-Spatio-Temporal-Energy-Graph">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>


<tr>
<td style="width: 28px;">&nbsp;4</td>
<td style="width: 172px;"><strong>Explainable and Explicit Visual Reasoning over Scene Graphs  (CVPR 2019)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1812.01855" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/shijx12/XNM-Net">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>


<tr>
<td style="width: 28px;">&nbsp;5</td>
<td style="width: 172px;"><strong>MUREL: Multimodal Relational Reasoning for Visual Question Answering  (CVPR 2019)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1902.09487" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/Cadene/murel.bootstrap.pytorch">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>


<tr>
<td style="width: 28px;">&nbsp;6</td>
<td style="width: 172px;"><strong>RAVEN: A Dataset for Relational and Analogical Visual Reasoning  (CVPR 2019)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1903.02741">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="http://wellyzhang.github.io/project/raven.html">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>


<tr>
<td style="width: 28px;">&nbsp;7</td>
<td style="width: 172px;"><strong>Explainable and Explicit Visual Reasoning over Scene Graphs  (CVPR 2019)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1812.01855" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/shijx12/XNM-Net">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>


<tr>
<td style="width: 28px;">&nbsp;8</td>
<td style="width: 172px;"><strong>BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection
(AAAI 2019)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1902.00038">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/Cadene/block.bootstrap.pytorch">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>


<tr>
<td style="width: 28px;">&nbsp;9</td>
<td style="width: 172px;"><strong>Dynamic Capsule Attention for Visual Question Answering (AAAI 2019)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-ZhouYiyi2.3610.pdf">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/XMUVQA/CapsAtt">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>

<tr>
<td style="width: 28px;">&nbsp;10</td>
<td style="width: 172px;"><strong>Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering
(AAAI 2019)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://www.semanticscholar.org/paper/Beyond-RNNs%3A-Positional-Self-Attention-with-for-Li-Song/565359aac8914505e6b02db05822ee63d3ffd03a" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/lixiangpengcs/PSAC">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>

<tr>
<td style="width: 28px;">&nbsp;11</td>
<td style="width: 172px;"><strong>Free VQA Models from Knowledge Inertia by Pairwise Inconformity Learning (AAAI 2019)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-ZhouYiyi1.1233.pdf" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/xiangmingLi/PIL">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>

--
<tr>
<td style="width: 28px;">&nbsp;12</td>
<td style="width: 172px;"><strong>Focal Visual-Text Attention for Memex Question Answering (TPAMI 2019)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://ieeexplore.ieee.org/abstract/document/8603827/" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://memexqa.cs.cmu.edu/">[Code]</a></td>
<td style="width: 55px;">2019</td>
</tr>

<tr>
<td style="width: 28px;">&nbsp;13</td>
<td style="width: 172px;"><strong>Co-Attending Free-Form Regions and Detections with Multi-Modal Multiplicative Feature Embedding for Visual Question Answering (AAAI 2018)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16249" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/lupantech/dual-mfa-vqa/">[Code]</a></td>
<td style="width: 55px;">2018</td>
</tr>
  
<tr>
<td style="width: 28px;">&nbsp;14</td>
<td style="width: 172px;"><strong>Textbook Question Answering Under Instructor Guidance With Memory Networks  (CVPR 2018)</strong></td>
<td style="width: 80px;">&nbsp;<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Li_Textbook_Question_Answering_CVPR_2018_paper.html" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/freerailway/igmn">[Code]</a></td>
<td style="width: 55px;">2018</td>
</tr> 
  
<tr>
<td style="width: 28px;">&nbsp;15</td>
<td style="width: 172px;"><strong>Explore Multi-Step Reasoning in Video Question Answering  (ACM MM 2018)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://doi.org/10.1145/3240508.3240563" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/SVQA-founder/SVQA/tree/master/code">[Code]</a></td>
<td style="width: 55px;">2018</td>
</tr> 
  

<tr>
<td style="width: 28px;">&nbsp;16</td>
<td style="width: 172px;"><strong>Visual Question Generation for Class Acquisition of Unknown Objects (ECCV 2018)</strong></td>
<td style="width: 80px;">&nbsp;<a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Kohei_Uehara_Visual_Question_Generation_ECCV_2018_paper.html" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/mil-tokyo/vqg-unknown">[Code]</a></td>
<td style="width: 55px;">2018</td>
</tr> 

<tr>
<td style="width: 28px;">&nbsp;17</td>
<td style="width: 172px;"><strong>A Better Way to Attend: Attention With Trees for Video Question Answering (TIP 2018)</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://ieeexplore.ieee.org/document/8419716">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/xuehy/TreeAttention">[Code]</a></td>
<td style="width: 55px;">2018</td>
</tr>  
  
<tr>
<td style="width: 28px;">&nbsp;18</td>
<td style="width: 172px;">Yan Zhang, Jonathon Hare, Adam Pr&uuml;gel-Bennett:&nbsp;<strong>Learning to Count Objects in Natural Images for Visual Question Answering</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1802.05766" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/Cyanogenoid/vqa-counting">[Code]</a></td>
<td style="width: 55px;">2018</td>
</tr>
<tr>
<td style="width: 28px;">&nbsp;19</td>
<td style="width: 172px;">Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh,&nbsp;<strong>VQA: Visual Question Answering</strong>, ICCV, 2015.</td>
<td style="width: 80px;"><a href="http://arxiv.org/pdf/1505.00468" >[Paper]</a></td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/JamesChuanggg/VQA-tensorflow">[Code]</a></td>
<td style="width: 55px;">2015</td>
</tr>
<tr>
<td style="width: 28px;">20&nbsp;</td>
<td style="width: 172px;">Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola,&nbsp;<strong>Stacked Attention Networks for Image Question Answering</strong>, CVPR 2016.&nbsp;</td>
<td style="width: 80px;"><a href="http://arxiv.org/abs/1511.02274">[Paper]</a></td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/JamesChuanggg/san-torch">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">21&nbsp;</td>
<td style="width: 172px;">
<p>Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh,&nbsp;<strong>Hierarchical Question-Image Co-Attention for Visual Question Answering</strong>, arXiv:1606.00061, 2016.&nbsp;&nbsp;</p>
</td>
<td style="width: 80px;"><a href="https://arxiv.org/pdf/1606.00061v2.pdf" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/jiasenlu/HieCoAttenVQA">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">22&nbsp;</td>
<td style="width: 172px;">
<p>Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach,&nbsp;<strong>Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</strong>, arXiv:1606.01847, 2016.&nbsp;</p>
</td>
<td style="width: 80px;"><a href="https://arxiv.org/abs/1606.01847" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/akirafukui/vqa-mcb">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">23&nbsp;</td>
<td style="width: 172px;">
<p>Vahid Kazemi, Ali Elqursh,&nbsp;<strong>Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering</strong>, arXiv:1704.03162, 2016.&nbsp;</p>
</td>
<td style="width: 80px;"><a href="https://arxiv.org/abs/1704.03162" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/Cyanogenoid/pytorch-vqa">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">24&nbsp;</td>
<td style="width: 172px;">
<p>Hedi Ben-younes, Remi Cadene, Matthieu Cord, Nicolas Thome:&nbsp;<strong>MUTAN: Multimodal Tucker Fusion for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/pdf/1705.06676.pdf" >[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/Cadene/vqa.pytorch">[Code]</a></td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">25&nbsp;</td>
<td style="width: 172px;">
<p><strong>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/pdf/1502.05698v1.pdf" >[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/facebook/bAbI-tasks">[Code]</a></td>
<td style="width: 55px;">2015</td>
</tr>
<tr>
<td style="width: 28px;">26&nbsp;</td>
<td style="width: 172px;"><strong>Neural Module Networks</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/pdf/1502.05698v1.pdf" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://arxiv.org/pdf/1502.05698v1.pdf" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">27&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/pdf/1511.05756" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/HyeonwooNoh/DPPnet" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2015</td>
</tr>
<tr>
<td style="width: 28px;">28&nbsp;</td>
<td style="width: 172px;">
<p><strong>Stacked Attention Networks for Image Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="http://arxiv.org/abs/1511.02274" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;"><a href="https://github.com/abhshkdz/neural-vqa-attention">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">29&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Simple Baseline for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="http://arxiv.org/abs/1512.02167" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/metalbubble/VQAbaseline" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2015</td>
</tr>
<tr>
<td style="width: 28px;">30&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Question Answering via Integer Programming over Semi-Structured Knowledge</strong></p>
</td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="http://arxiv.org/abs/1604.06076" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/allenai/tableilp" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">31&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Hierarchical Question-Image Co-Attention for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="http://arxiv.org/abs/1606.00061" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/jiasenlu/HieCoAttenVQA" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">32&nbsp;</td>
<td style="width: 172px;">
<p><strong>Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</strong></p>
</td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="https://arxiv.org/abs/1606.01847" rel="nofollow">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/akirafukui/vqa-mcb" rel="nofollow">[Code]</a></td>
<td style="width: 55px;">2016</td>
</tr>
<tr>
<td style="width: 28px;">33&nbsp;</td>
<td style="width: 172px;"><strong>Hadamard Product for Low-rank Bilinear Pooling</strong></td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="https://arxiv.org/abs/1610.04325" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/jnhwkim/MulLowBiVQA" rel="nofollow">[Code]</a>&nbsp;</td>
<td style="width: 55px;">
<p>2017</p>
</td>
</tr>
<tr>
<td style="width: 28px;">34&nbsp;</td>
<td style="width: 172px;"><strong>TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering</strong></td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="https://arxiv.org/abs/1704.04497" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/YunseokJANG/tgif-qa" rel="nofollow">[Code]</a>&nbsp;</td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">35&nbsp;</td>
<td style="width: 172px;"><strong>Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks</strong></td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="https://arxiv.org/abs/1704.08384" rel="nofollow">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/rajarshd/TextKBQA" rel="nofollow">[Code]</a>&nbsp;</td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">36&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Learning Convolutional Text Representations for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="https://arxiv.org/abs/1705.06824">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/divelab/vqa-text">[Code]</a></td>
<td style="width: 55px;">2018</td>
</tr>
<tr>
<td style="width: 28px;">37&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1707.07998">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com//peteanderson80/bottom-up-attention">[Code]</a>&nbsp;</td>
<td style="width: 55px;">2018</td>
</tr>
<tr>
<td style="width: 28px;">38&nbsp;</td>
<td style="width: 172px;">
<p><strong>Structured Attentions for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1708.02071">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/zhuchen03/vqa-sva">[Code]</a></td>
<td style="width: 55px;">&nbsp;2017</td>
</tr>
<tr>
<td style="width: 28px;">39&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Question Dependent Recurrent Entity Network for Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1707.07922">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/andreamad8/QDREN">[Code]</a></td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">40&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>DCN+: Mixed Objective and Deep Residual Coattention for Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1707.07922">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://arxiv.org/abs/1707.07922">[Code]</a></td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">41&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Embodied Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;&nbsp;<a href="https://arxiv.org/abs/1711.11543">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;&nbsp;<a href="https://github.com/facebookresearch/EmbodiedQA">[Code]</a></td>
<td style="width: 55px;">2017</td>
</tr>
<tr>
<td style="width: 28px;">42&nbsp;</td>
<td style="width: 172px;">&nbsp;
<p><strong>Structured Triplet Learning with POS-tag Guided Attention for Visual Question Answering</strong></p>
</td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1801.07853">[Paper]</a></td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/wangzheallen/STL-VQA">[Code]</a></td>
<td style="width: 55px;">2018</td>
</tr>
<tr>
<td style="width: 28px;">43&nbsp;</td>
<td style="width: 172px;"><strong>Bilinear Attention Networks</strong></td>
<td style="width: 80px;">&nbsp;<a href="https://arxiv.org/abs/1805.07932">[Paper]</a>&nbsp;</td>
<td style="width: 70px; text-align: center;">&nbsp;<a href="https://github.com/jnhwkim/ban-vqa">[Code]</a>&nbsp;</td>
<td style="width: 55px;">2018</td>
</tr>

</tbody>
</table>

<th>
<p><strong><span style="color: red;">Projects</span></strong></p>
<table style="height: 147px; width: 440px;">
<tbody>
<tr>
<td style="width: 29px;">&nbsp;</td>
<td style="width: 258px; text-align: center;"><strong>&nbsp;Title</strong></td>
<td style="width: 141px; text-align: center;"><strong>Code&nbsp;</strong></td>
</tr>
<tr>
<td style="width: 29px;">&nbsp;1</td>
<td style="width: 258px;"><strong>VQA Demo: Visual Question Answering Demo on pretrained model</strong></td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/iamaaditya/VQA_Demo">[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">2&nbsp;</td>
<td style="width: 258px;"><strong>Deep-QA: Implementation of the Convolution Neural Network for factoid QA on the answer sentence selection task</strong></td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/aseveryn/deep-qa" >[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">3&nbsp;</td>
<td style="width: 258px;"><strong>InsuranceQA-CNN-LSTM: Tensorflow and Theano CNN code for insurance QA(question Answer matching)</strong></td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/white127/insuranceQA-cnn-lstm">[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">4&nbsp;</td>
<td style="width: 258px;">&nbsp;
<p><strong>Tensorflow Implementation of Deeper LSTM+ normalized CNN for Visual Question Answering</strong></p>
</td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/JamesChuanggg/VQA-tensorflow" >[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">5&nbsp;</td>
<td style="width: 258px;">&nbsp;
<p><strong>Visual Question Answering with Keras</strong></p>
</td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering" >[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">6&nbsp;</td>
<td style="width: 258px;">
<p><strong>Visual Question Answering in Pytorch</strong></p>
</td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/Cadene/vqa.pytorch" >[Code]</a>&nbsp;</td>
</tr>
<tr>
<td style="width: 29px;">7&nbsp;</td>
<td style="width: 258px;">
<p><strong>Deep QA: Using deep learning to answer Aristo&rsquo;s science questions</strong></p>
</td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/allenai/deep_qa" >[Code]</a>&nbsp;</td>
</tr>

<tr>
<td style="width: 29px;">8&nbsp;</td>
<td style="width: 258px;">
<p><strong>CNN, LSTM + CNN, SWEM on Insurance-QA data</strong></p>
</td>
<td style="width: 141px;">&nbsp;<a href="https://github.com/white127/QA-deep-learning" >[Code]</a>&nbsp;</td>
</tr>




</tbody>
</table>
<th>
<p><strong><span style="color: red;">Datasets</span></strong></p>

<table>
<tr>
<th></th>
<th>Title</th>
<th>Link</th>
</tr>
  
<tr>
<td>1</td>
<td>VQA-v1 Dataset</td>
<td><a href="https://visualqa.org/vqa_v1_download.html">Link</a></td>
</tr>

<tr>
<td>2</td>
<td>VQA-v2 Dataset</td>
<td><a href="https://visualqa.org/download.html">Link</a></td>
</tr>


<tr>
<td>3</td>
<td>VizWiz-VQA v1.0</td>
<td><a href="https://vizwiz.org/download/245/">Link</a></td>
</tr>

<tr>
<td>4</td>
<td>VizWiz-VQA v2.0</td>
<td><a href="https://vizwiz.org/download/247/">Link</a></td>
</tr>

<tr>
<td>4</td>
<td>TDIUC dataset</td>
<td><a href="https://kushalkafle.com/projects/tdiuc">Link</a></td>
</tr>

<tr>
<td>4</td>
<td>MemexQA dataset</td>
<td><a href="https://memexqa.cs.cmu.edu/#dataset">Link</a></td>
</tr>


</table>
